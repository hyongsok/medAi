{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import configparser\n",
    "import csv\n",
    "import copy\n",
    "import numpy as np\n",
    "import imageio\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from RetinaCheckerCSV import RetinaCheckerCSV\n",
    "from helper_functions import reduce_to_2_classes, AverageMeter, AccuracyMeter\n",
    "from make_default_config import get_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "config = get_config()\n",
    "config['network']['model'] = 'inception_v3'\n",
    "config['network']['pretrained'] = 'False'\n",
    "config['network']['optimizer'] = 'Adam'\n",
    "config['network']['criterion'] = 'BCEWithLogitsLoss'\n",
    "config['network']['multiclass'] = 'True'\n",
    "\n",
    "config['files']['label file'] = 'D:/Dropbox/Data/cropped-outer-test/labels.csv'\n",
    "config['files']['test path'] = 'D:/Dropbox/Data/cropped-outer-test/'\n",
    "config['files']['image size'] = '299'\n",
    "config['input']['checkpoint'] = 'model_181112_inc_csv_after_epoch_21.ckpt'\n",
    "config['input']['evaluation only'] = 'True'\n",
    "\n",
    "number_sample_images = 36\n",
    "max_batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = RetinaCheckerCSV()\n",
    "if config['network']['model'] == 'inception_v3':\n",
    "    rc.model_kwargs['aux_logits'] = False\n",
    "rc.initialize( config )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.load_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=rc.test_dataset,\n",
    "                                        batch_size=min(len(rc.test_dataset), max_batch_size),\n",
    "                                        shuffle=True,\n",
    "                                        sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = np.zeros((len(rc.test_dataset), len(rc.classes)))\n",
    "all_labels = np.zeros_like(all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rc.model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    losses = AverageMeter()\n",
    "    accuracy = AccuracyMeter()\n",
    "\n",
    "    confusion = torch.zeros((2, 2), dtype=torch.float)\n",
    "    confusion_old = torch.zeros((5, 5), dtype=torch.float)\n",
    "    counter = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        print('.', end='', flush=True)\n",
    "        images = images.to(rc.device)\n",
    "        labels = labels.to(rc.device)\n",
    "\n",
    "        outputs = rc.model(images)\n",
    "        loss = rc.criterion(outputs, labels)\n",
    "        \n",
    "        all_outputs[counter:counter+len(images), :] = outputs\n",
    "        all_labels[counter:counter+len(images), :] = labels\n",
    "        counter += len(images)\n",
    "\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "\n",
    "        num_correct = rc._evaluate_performance( labels, outputs )\n",
    "\n",
    "        accuracy.update(num_correct, labels.size(0))\n",
    "        predicted = torch.nn.Sigmoid()(outputs)\n",
    "        \n",
    "        for pred, lab in zip(predicted[:,5].round().cpu().numpy().astype(np.int), labels[:,5].cpu().numpy().astype(np.int)):\n",
    "            confusion[pred, lab] += 1\n",
    "        for pred, lab in zip(predicted[:,:5].argmax(1), labels[:,:5].argmax(1)):\n",
    "            confusion_old[pred, lab] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Accuracy of the model on the {} test images: {} %'.format(accuracy.count, accuracy.avg*100))\n",
    "print('Classes: {}'.format(5))\n",
    "print('Confusion matrix:\\n', (confusion_old))\n",
    "\n",
    "confusion_2class = confusion\n",
    "print('Accuracy: {:.1f}%'.format(np.diag(confusion_2class).sum()/confusion_2class.sum()*100))\n",
    "print(confusion_2class)\n",
    "print('Sensitivity: {:.1f}%'.format(confusion_2class[1,1]/confusion_2class[:,1].sum()*100))\n",
    "print('Specificity: {:.1f}%'.format(confusion_2class[0,0]/confusion_2class[:,0].sum()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "max_samples = min(len(rc.test_dataset), number_sample_images)\n",
    "\n",
    "if max_samples < len(rc.test_dataset):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=max_samples)\n",
    "    sampling_index = next(iter(sss.split([img[0] for img in rc.test_dataset.imgs], [img[1][:5].argmax() for img in rc.test_dataset.imgs])))[1]\n",
    "else:\n",
    "    sampling_index = np.arange(len(rc.test_dataset), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = copy.deepcopy(rc.test_dataset)\n",
    "dataset.imgs = [dataset.imgs[ii] for ii in sampling_index]\n",
    "dataset.samples = dataset.imgs\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                    batch_size=min(len(dataset), max_batch_size),\n",
    "                                    shuffle=False,\n",
    "                                    sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "font_size = 8\n",
    "if number_sample_images > 0:  \n",
    "    n_cols = 6\n",
    "    n_rows = np.ceil(number_sample_images/n_cols)\n",
    "    classlabel = ['no DMR', 'mild NPDR', 'mod NPDR', 'severe NPDR', 'PDR', '>=mod', '>=severe']\n",
    "    fig, ax = plt.subplots(int(n_rows), int(n_cols), True, True, figsize=(20,int(n_rows)*3))\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(rc.device)\n",
    "            labels = labels.to(rc.device)\n",
    "\n",
    "            outputs = rc.model(images)\n",
    "\n",
    "            for img, lab, out in zip(images, labels, outputs):\n",
    "                ii = int(counter/n_cols)\n",
    "                jj = int(counter%n_cols)\n",
    "                img_array = skimage.transform.resize(imageio.imread(dataset.imgs[counter][0]), (224,224), mode='constant', anti_aliasing=True)\n",
    "                ax[ii, jj].imshow(img_array, origin='lower')\n",
    "                ax[ii, jj].annotate( classlabel[lab[:5].argmax()], xy=(10,10), color='white', size=font_size)\n",
    "                prediction = torch.nn.Sigmoid()(out)\n",
    "                if prediction[5] > 0.5:\n",
    "                    ax[ii, jj].annotate( '>=mod', xy=(120,10), color='cyan', size=font_size)\n",
    "                else:\n",
    "                    ax[ii, jj].annotate( 'not ref', xy=(120,10), color='cyan', size=font_size)\n",
    "                if (lab[:5].argmax() < 2 and prediction[5] >= 0.5) or (lab[:5].argmax() >= 2 and prediction[5] < 0.5):\n",
    "                    ax[ii, jj].annotate( 'X', xy=(200,200), color='red', size=20)\n",
    "                for cc in range(7):\n",
    "                    ax[ii, jj].annotate( '{}: {:.3f}'.format(classlabel[cc], prediction[cc]) , xy=(10,205-cc*2*font_size), color='white', size=font_size)\n",
    "                counter+=1\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = all_labels\n",
    "y_score = all_outputs\n",
    "n_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 3\n",
    "n_rows = int(np.ceil(n_classes/n_cols))\n",
    "fig, ax = plt.subplots(n_rows, n_cols, True, True, figsize=(20,6*n_rows))\n",
    "lw = 2\n",
    "for ii in range(n_classes):\n",
    "    row = int(ii/n_cols)\n",
    "    col = int(ii%n_cols)\n",
    "    ax[row, col].plot(fpr[ii], tpr[ii], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[ii])\n",
    "    ax[row, col].plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax[row, col].set_xlim([-0.05, 1.05])\n",
    "    ax[row, col].set_ylim([-0.05, 1.05])\n",
    "    ax[row, col].set_xlabel('False Positive Rate')\n",
    "    ax[row, col].legend(loc=\"lower right\")\n",
    "    ax[row, col].set_title(dataset.classes[ii])\n",
    "    ax[row, 0].set_ylabel('True Positive Rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
